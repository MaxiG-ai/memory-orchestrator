"""
Insight Store - Vector store for semantic search over interaction summaries.

Stores Observer LLM summaries with embeddings for similarity search.
This is the "reasoning" component of the dual-store architecture.
"""

from typing import Any, Dict, List, Optional
import numpy as np

from memorch.utils.logger import get_logger

logger = get_logger("InsightStore")


class InsightStore:
    """
    Vector store for interaction summaries using FlagEmbedding.

    Stores natural language summaries generated by the Observer LLM,
    embedded for semantic similarity search. Returns trace_ids that
    can be used to fetch raw data from the FactStore.
    """

    def __init__(self, embedding_model: Any):
        """
        Initialize with an embedding model.

        Args:
            embedding_model: A FlagModel instance (or mock with .encode() method)
        """
        self._model = embedding_model
        self._entries: List[Dict] = []  # [{trace_id, summary, embedding}, ...]

    def add(self, trace_id: str, summary: str) -> None:
        """
        Add a summary with its embedding to the store.

        Args:
            trace_id: UUID linking to FactStore record
            summary: Natural language summary from Observer LLM
        """
        embedding = self._model.encode([summary])[0]
        self._entries.append(
            {
                "trace_id": trace_id,
                "summary": summary,
                "embedding": np.array(embedding),
            }
        )

    def search(self, query: str, top_k: int = 3) -> List[str]:
        """
        Search for top-K most similar summaries and return their trace_ids.

        Args:
            query: Search query (e.g., "how to proceed")
            top_k: Maximum number of results to return

        Returns:
            List of trace_ids ordered by descending similarity
        """
        if not self._entries:
            return []

        # Embed query
        query_embedding = self._model.encode([query])[0]
        query_embedding = np.array(query_embedding)

        # Build matrix of stored embeddings
        stored_embeddings = np.array([e["embedding"] for e in self._entries])

        # Compute cosine similarities via dot product (embeddings assumed normalized)
        # For non-normalized embeddings, we normalize here
        query_norm = query_embedding / (np.linalg.norm(query_embedding) + 1e-9)
        stored_norms = stored_embeddings / (
            np.linalg.norm(stored_embeddings, axis=1, keepdims=True) + 1e-9
        )
        similarities = stored_norms @ query_norm

        # Get top-K indices (descending similarity)
        top_indices = np.argsort(similarities)[::-1][:top_k]

        return [self._entries[i]["trace_id"] for i in top_indices]

    def get_summary(self, trace_id: str) -> Optional[str]:
        """
        Get the summary for a specific trace_id.

        Args:
            trace_id: UUID to look up

        Returns:
            Summary string or None if not found
        """
        for entry in self._entries:
            if entry["trace_id"] == trace_id:
                return entry["summary"]
        return None

    def is_empty(self) -> bool:
        """Check if store has any entries."""
        return len(self._entries) == 0

    def clear(self) -> None:
        """Remove all entries. Called on session reset."""
        self._entries.clear()

    def size(self) -> int:
        """Return number of stored entries."""
        return len(self._entries)
